# Introduction
I created a contrived example where the goal is to calculate the average values of soil moisture and soil temp
within a specific time interval, namely the period between planting date and emergence date, for each cropID.

We have two tables:

- `soil`: This is a soil table with columns `lat`, `lon`, `date`, `soilTemp`, and `soilMoisture`.
- `GS`: This is a growth stage table with columns `lat`, `lon`, `cropID`, `fieldID`, `plantingDate`, `emergenceDate`, and some other primary growth stages.


## Simulation

The Simulator object contains two functions, createGSDF and createSoilDF, which are used to create data frames for soil and growth stages (GS) respectively.

The `createGSDF` function generates a data frame for GS with columns for latitude (`lat`), longitude (`lon`), crop ID (`cropID`), planting date (`plantingDate`), and emergence date (`emergenceDate`). It takes in several parameters including the number of rows, number of unique crop IDs, number of unique sites, start and end dates, and returns the GS data frame.

The `createSoilDF` function generates a data frame for soil with columns for latitude (`lat`), longitude (`lon`), date, soil moisture, and soil temperature. It takes in the GS data frame generated by `createGSDF`, start and end dates, and returns the soil data frame. This function first generates a sequence of dates using `getDateRange` function from the `Utils` object. It then joins the sequence of dates with the GS data frame, creating a new data frame that combines the unique latitudes and longitudes with each date in the range. Finally, it adds randomly generated values for soil moisture and temperature for each latitude, longitude, and date combination.


```scala
package similator


import org.apache.spark.sql.{DataFrame, Row, SparkSession}
import org.apache.spark.sql.types._
import org.apache.spark.sql.functions.{col, rand, explode}

import java.time.format.DateTimeFormatter
import java.time.{LocalDate, ZoneOffset}
import java.time.format.DateTimeFormatter
import java.time.{Duration, LocalDate}

import utils.Utils._

object Similator {
  // declaration and definition of function
  def createGSDF(
      spark: SparkSession,
      numRows: Int,
      numUniqueCropIds: Int,
      numUniqueSites: Int,
      startDate: LocalDate,
      endDate: LocalDate
  ): DataFrame = {

    // Create the gsDF data frame
    val gsRows: Seq[Row] = (1 to numRows).map { i =>
      val lat: Double =
        39.876887 + (i % numUniqueSites - numUniqueSites / 2) * 0.01
      val lon: Double =
        -105.03242 + (i % numUniqueSites - numUniqueSites / 2) * 0.01
      val cropId: Int = i % numUniqueCropIds + 1
      // format the dates, this converts from LocalDate to String
      val formatter: DateTimeFormatter =
        DateTimeFormatter.ofPattern("yyyy-MMM-dd")
      val plantingDate: String = startDate
        .plusDays(
          scala.util.Random.nextInt(
            endDate.toEpochDay.toInt - startDate.toEpochDay.toInt + 1
          )
        )
        .format(formatter)
      val emergenceDate: String =
        LocalDate
          .parse(plantingDate)
          .plusDays(scala.util.Random.nextInt(20) + 1)
          .format(formatter)
      Row(lat, lon, cropId, plantingDate, emergenceDate)
    }
    val gsSchema: StructType = StructType(
      Seq(
        StructField("lat", DoubleType, nullable = false),
        StructField("lon", DoubleType, nullable = false),
        StructField("cropID", IntegerType, nullable = false),
        StructField("plantingDate", StringType, nullable = false),
        StructField("emergenceDate", StringType, nullable = false)
      )
    )
    val gsDF: DataFrame =
      spark.createDataFrame(spark.sparkContext.parallelize(gsRows), gsSchema)

    gsDF
  }

  def createSoilDF(
      spark: SparkSession,
      gsDF: DataFrame,
      startDate: LocalDate,
      endDate: LocalDate
  ): DataFrame = {

    // format the dates, this converts from LocalDate to String
    val formatter: DateTimeFormatter =
      DateTimeFormatter.ofPattern("yyyy-MMM-dd")

    // without this line toDF won't work
    import spark.implicits._
    // Generate dates between startDate and endDate
    val minMaxWithRange = getDateRange(startDate, endDate).toDF("range")

    val dates = minMaxWithRange
      .withColumn("date", explode(col("range")))
      .drop("range")

    //  Join the sequence of data with gsDF dataframe
    val soilDF: DataFrame = gsDF
      .select(col("lat").as("lat"), col("lon").as("lon"))
      .distinct()
      .crossJoin(dates)
      .withColumn("soilMoisture", rand())
      .withColumn("soilTemp", rand(18) * 6 + 18)


    // Return the soilDF data frame
    soilDF
  }

}

```

### GS Dataframe

| lat      | lon       | cropID | plantingDate | emergenceDate |
|----------|-----------|--------|--------------|---------------|
| 39.87689 | -105.0324 | 1      | 2022-Jan-04  | 2022-Jan-12   |
| 38.0000  | -100.0314 | 2      | 2022-Jan-20  | 2022-Feb-02   |
| 40.84300 | -102.1374 | 3      | 2022-Jan-12  | 2022-Jan-29   |
| ...      | ...       | ...    | ...          | ...           |



### Soil Dataframe

| lat      | lon       | date       | soilMoisture | soilTemp |
|----------|-----------|------------|--------------|----------|
| 39.87689 | -105.0324 | 2022-01-01 | 0.318        | 20.063   |
| 39.87689 | -105.0324 | 2022-01-02 | 0.762        | 18.944   |
| 39.87689 | -105.0324 | 2022-01-03 | 0.436        | 23.832   |
| ...      | ...       | ...        | ...          | ...      |


## Spark and Scala for Dataframe Operations
`emergence.scala` performs an analysis of the emergence rate of crops based on soil data. It first creates a growth stage data frame and a soil data frame using the Simulator object's createGSDF and createSoilDF functions. It then joins the two data frames based on the "lat" and "lon" columns, calculates the difference between the "emergenceDate" and "plantingDate" columns to create a new "emergenceDays" column, and categorizes the emergence rate as "fast", "average", or "slow" based on the number of days it takes for emergence to occur. It then groups the result by the "cropID" and "emergenceRate" columns and aggregates the mean soil temperature and moisture for each group.

```scala
import org.apache.spark.sql.{DataFrame, Row, SparkSession}
import org.apache.spark.sql.types._
import org.apache.spark.sql.functions.{
  datediff,
  to_date,
  col,
  when,
  mean,
  filter
}

import java.time.{LocalDate, ZoneOffset, Period}

import similator.Similator._

object Emergence {
  def main(args: Array[String]) {

    // Create the SparkSession object
    val spark: SparkSession = SparkSession
      .builder()
      .appName("Emergence")
      .master("local[*]")
      .getOrCreate()

    // Set the log level to WARN
    spark.sparkContext.setLogLevel("WARN")

    // Create the growth stage data frame with 10000 rows, 10 types of crops, and 100 sites
    val gsDF: DataFrame = createGSDF(
      spark,
      10000,
      10,
      100,
      LocalDate.of(2017, 1, 1),
      LocalDate.of(2023, 1, 1)
    )

    // Create the soil data frame
    val soilDF: DataFrame = createSoilDF(
      spark,
      gsDF,
      LocalDate.of(2017, 1, 1),
      LocalDate.of(2023, 1, 1)
    )

    // Join the soil data and growth stage data on the "lat" and "lon" columns
    val joinedDF = soilDF.join(gsDF, Seq("lat", "lon"))

    // Create a new column "emergenceDays" that is the difference between the "emergenceDate" and "plantingDate" columns
    val withEmergenceDaysDF = joinedDF.withColumn(
      "emergenceDays",
      datediff(
        to_date(col("emergenceDate"), "yyyy-MMM-dd"),
        to_date(col("plantingDate"), "yyyy-MMM-dd")
      )
    )

    // Create a new column "emergenceRate"
    // that indicates whether the emergence rate is fast, average, or slow
    val withEmergenceRate = withEmergenceDaysDF.withColumn(
      "emergenceRate",
      when(col("emergenceDays") > 10, "slow")
        .when(col("emergenceDays") < 5, "fast")
        .otherwise("fast")
    )

    // Group the result by the "cropID" and "emergenceRate" columns and aggregate on soil values
    val resultDF = withEmergenceRate
      .filter(
        to_date(col("date"), "yyyy-MMM-dd").between(
          to_date(col("plantingDate"), "yyyy-MMM-dd"),
          to_date(col("emergenceDate"), "yyyy-MMM-dd")
        )
      )
      .groupBy("cropID", "emergenceRate")
      .agg(
        mean("soilTemp").as("avgSoilTemp"),
        mean("soilMoisture").as("avgSoilMoisture")
      )

    // Select the "cropID", "emergenceRate", "avgSoilTemp", and "avgSoilMoisture" columns from the result data
    val selectedDF = resultDF.select(
      "cropID",
      "emergenceRrate",
      "avgSoilTemp",
      "avgSoilMoisture"
    )

    // Display the resulting data
    selectedDF.show()

    // spark.stop()
  }
}
```
